---
layout: archive
title: "Publications"
permalink: /publications/
author_profile: true
---

{% if author.googlescholar %}
  You can also find my articles on <u><a href="{{author.googlescholar}}">my Google Scholar profile</a>.</u>
{% endif %}

{% include base_path %}

[Less is More: Task-aware Layer-wise Distillation for Language Model Compression](https://arxiv.org/abs/2210.01351) <br>
Chen Liang, **Simiao Zuo**, Qingru Zhang, Pengcheng He, Weizhu Chen and Tuo Zhao <br>
*arxiv*, 2022

[DiP-GNN: Discriminative Pre-Training of Graph Neural Networks](https://arxiv.org/abs/2209.07499) <br>
**Simiao Zuo**, Haoming Jiang, Qingyu Yin, Xianfeng Tang, Bing Yin and Tuo Zhao <br>
*arxiv*, 2022

[Differentially Private Estimation of Hawkes Process](https://arxiv.org/abs/2209.07303) <br>
**Simiao Zuo**, Tianyi Liu, Tuo Zhao and Hongyuan Zha <br>
*arxiv*, 2022

[Context-Aware Query Rewriting for Improving Users' Search Experience on E-commerce Websites](https://arxiv.org/abs/2209.07584)<br>
**Simiao Zuo**, Qingyu Yin, Haoming Jiang, Shaohui Xi, Bing Yin, Chao Zhang and Tuo Zhao <br>
*arxiv*, 2022

[PLATON: Pruning Large Transformer Models with Upper Confidence Bound of Weight Importance](https://arxiv.org/abs/2206.12562) [[code]](https://github.com/QingruZhang/PLATON) <br>
Qingru Zhang, **Simiao Zuo**, Chen Liang, Alexander Bukharin, Pengcheng He, Weizhu Chen and Tuo Zhao <br>
*International Conference on Machine Learning (ICML)*, 2022

[MoEBERT: from BERT to Mixture-of-Experts via Importance-Guided Adaptation](https://arxiv.org/abs/2204.07675) [[code]](https://github.com/SimiaoZuo/MoEBERT) <br>
**Simiao Zuo**, Qingru Zhang, Chen Liang, Pengcheng He, Tuo Zhao and Weizhu Chen <br>
*North American Chapter of the Association for Computational Linguistics (NAACL)*, 2022

[Self-Training with Differentiable Teacher](https://arxiv.org/abs/2109.07049) <br>
**Simiao Zuo**\*, Yue Yu\*, Chen Liang, Haoming Jiang, Siawpeng Er, Chao Zhang, Tuo Zhao and Hongyuan Zha <br>
*Findings of North American Chapter of the Association for Computational Linguistics (NAACL)*, 2022

[Adversarially Regularized Policy Learning Guided by Trajectory Optimization](https://arxiv.org/abs/2109.07627) <br>
Zhigen Zhao, **Simiao Zuo**, Tuo Zhao and Ye Zhao <br>
*Annual Learning for Dynamics & Control Conference (L4DC)*, 2022

[No Parameters Left Behind: Sensitivity Guided Adaptive Learning Rate for Training Large Transformer Models](https://arxiv.org/abs/2202.02664) [[code]](https://github.com/cliang1453/SAGE) <br>
Chen Liang, Haoming Jiang, **Simiao Zuo**, Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen and Tuo Zhao <br>
*International Conference on Learning Representations (ICLR)*, 2022

[Taming Sparsely Activated Transformer with Stochastic Experts](https://arxiv.org/abs/2110.04260) [[code]](https://github.com/microsoft/Stochastic-Mixture-of-Experts) <br>
**Simiao Zuo**, Xiaodong Liu, Jian Jiao, Young Jin Kim, Hany Hassan, Ruofei Zhang, Tuo Zhao and Jianfeng Gao <br>
*International Conference on Learning Representations (ICLR)*, 2022

[Adversarial Regularization as Stackelberg Game: An Unrolled Optimization Approach](http://arxiv.org/abs/2104.04886) [[code]](https://github.com/SimiaoZuo/Stackelberg-Adv) <br>
**Simiao Zuo**, Chen Liang, Haoming Jiang, Xiaodong Liu, Pengcheng He, Jianfeng Gao, Weizhu Chen and Tuo Zhao <br>
*Empirical Methods in Natural Language Processing (EMNLP)*, 2021

[ARCH: Efficient Adversarial Regularized Training with Caching](https://arxiv.org/abs/2109.07048) [[code]](https://github.com/SimiaoZuo/Caching-Adv) <br>
**Simiao Zuo**, Chen Liang, Haoming Jiang, Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen and Tuo Zhao <br>
*Findings of Empirical Methods in Natural Language Processing (EMNLP)*, 2021

[Super Tickets in Pre-Trained Language Models: From Model Compression to Improving Generalization](https://arxiv.org/abs/2105.12002) [[code]](https://github.com/cliang1453/super-structured-lottery-tickets) <br>
Chen Liang, **Simiao Zuo**, Minshuo Chen, Haoming Jiang, Xiaodong Liu, Pengcheng He, Tuo Zhao and Weizhu Chen <br>
*Association for Computational Linguistics (ACL)*, 2021

[Fine-Tuning Pre-trained Language Model with Weak Supervision: A Contrastive-Regularized Self-Training Approach](https://arxiv.org/abs/2010.07835) [[code]](https://github.com/yueyu1030/COSINE) <br>
Yue Yu\*, **Simiao Zuo**\*, Haoming Jiang, Wendi Ren, Tuo Zhao and Chao Zhang <br>
*North American Chapter of the Association for Computational Linguistics (NAACL)*, 2021

[A Hypergradient Approach to Robust Regression without Correspondence](https://arxiv.org/abs/2012.00123) <br>
Yujia Xie\*, Yixiu Mao\*, **Simiao Zuo**, Hongteng Xu, Xiaojie Ye, Tuo Zhao and Hongyuan Zha <br>
*International Conference on Learning Representations (ICLR)*, 2021

[Transformer Hawkes Process](https://arxiv.org/abs/2002.09291) [[code]](https://github.com/SimiaoZuo/Transformer-Hawkes-Process) <br>
**Simiao Zuo**, Haoming Jiang, Zichong Li, Tuo Zhao and Hongyuan Zha <br>
*International Conference on Machine Learning (ICML)*, 2020

[Tensor maps for synchronizing heterogeneous shape collections](https://dl.acm.org/doi/abs/10.1145/3306346.3322944) <br>
Qixing Huang, Zhenxiao Liang, Haoyun Wang, **Simiao Zuo** and Chandrajit Bajaj <br>
*ACM Transactions on Graphics (TOG)*, 2019

